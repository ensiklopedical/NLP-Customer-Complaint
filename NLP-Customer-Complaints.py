# -*- coding: utf-8 -*-
"""Dicoding_NLP_Faisal_Ahmad_Gifari.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Vs9JSRI319OPC2HgEv3WIWVmHF6ZRnQs

# Data Diri

- Nama : Faisal Ahmad Gifari
- Jenis Kelamin : Laki-Laki
- Pekerjaaan : Mahasiswa
- Tempat/Tanggal Lahir : Kuningan, 17 September 2002
- Username : faisal_ag_037
- email : pd-20379543@edu.jakarta.go.id
- No. Telepon : 085775063559
- Kota Domisili : Jakarta Barat
- Institusi : UIN Syarif Hidayatullah Jakarta
"""

from google.colab import drive
drive.mount('/content/drive')

"""#Import Library dan Datasets"""

import pandas as pd
import re
import tensorflow as tf
import matplotlib
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import keras
import numpy as np
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

data = pd.read_csv("//content//drive//MyDrive//Datasets//complaints_processed.csv", converters={'reviews.text' : str})

data.narrative = data.narrative.astype(str)

data = data.dropna()

data

data['product'].unique()

categories =data['product'].value_counts().index
counts = data['product'].value_counts().values

plt.figure(figsize=(13, 8))
plt.bar(categories, counts)
plt.xlabel('Categories')
plt.ylabel('Counts')
plt.title('Count of Categorical Data')
plt.show()

data.info()

"""# Data Preprocessing"""

values_to_drop = ['credit_card', 'retail_banking']

for value in values_to_drop:
  index_rows = data[data['product'] == value].index
  data.drop(index_rows, inplace=True)

data = data.groupby('product').apply(lambda x: x.sample(min(len(x), 190000))).reset_index(drop=True)

data.info()

categories =data['product'].value_counts().index
counts = data['product'].value_counts().values

plt.figure(figsize=(13, 8))
plt.bar(categories, counts)
plt.xlabel('Categories')
plt.ylabel('Counts')
plt.title('Count of Categorical Data')
plt.show()

data = data.sample(frac=1)

data.info()

data

data.drop('Unnamed: 0', axis=1, inplace=True)

data

label = data['product'].values

label

narrative = data[['narrative']].copy()

narrative

nltk.download('wordnet')
def lemmatization(text):
    lm = WordNetLemmatizer()
    text = ' '.join([lm.lemmatize(word, pos='v') for word in text.split()])
    return text
narrative['narrative'] = narrative['narrative'].apply(lemmatization)

narrative.head()

narrative

# Download the stopwords from NLTK
nltk.download('punkt')
nltk.download('stopwords')

def remove_stopwords(text):
    stop_words = set(stopwords.words('english'))

    word_tokens = word_tokenize(text)

    filtered_text = [word for word in word_tokens if word.casefold() not in stop_words]

    return " ".join(filtered_text)

narrative['narrative'] = narrative['narrative'].apply(remove_stopwords)

narrative.head()

narrative

narrative_array = narrative['narrative'].values

narrative_array

"""bahaya

"""

nar_train, nar_test, label_train, label_test = train_test_split(narrative_array, label, test_size=0.2)

print(f"Jumlah data pada training set: {len(nar_train)}")
print(f"Jumlah data pada test set: {len(nar_test)}")

nar_train

nar_test

tokenizer = Tokenizer(num_words=5000, oov_token='x')
tokenizer.fit_on_texts(nar_train)

sequences_train = tokenizer.texts_to_sequences(nar_train)
sequences_test = tokenizer.texts_to_sequences(nar_test)

padded_train = pad_sequences(sequences_train)
padded_test = pad_sequences(sequences_test)

label_train = pd.get_dummies(label_train, columns=['product'])

label_train

label_test = pd.get_dummies(label_test, columns=['product'])

label_test

print(len(tokenizer.index_word) + 1 )

"""# Training"""

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim = len(tokenizer.index_word), output_dim=16),
    tf.keras.layers.LSTM(256),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(3, activation='softmax',)
])

model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

model.summary()

class EarlyStopper(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.901 and logs.get('val_accuracy')>0.901):
      print("Accuracy dan Val Accuracy sudah sama-sama mencapai 90%")
      self.model.stop_training = True

EarlyStopper1 = EarlyStopper()

class TrainingPlot(keras.callbacks.Callback):

    def on_train_begin(self, logs={}):
        self.losses = []
        self.acc = []
        self.val_losses = []
        self.val_acc = []
        self.logs = []

    def on_epoch_end(self, epoch, logs={}):

        self.logs.append(logs)
        self.losses.append(logs.get('loss'))
        self.acc.append(logs.get('accuracy'))
        self.val_losses.append(logs.get('val_loss'))
        self.val_acc.append(logs.get('val_accuracy'))

        if len(self.losses) > 1:

            N = np.arange(0, len(self.losses))

            plt.figure()
            plt.plot(N, self.losses, label = "train_loss")
            plt.plot(N, self.acc, label = "train_acc")
            plt.plot(N, self.val_losses, label = "val_loss")
            plt.plot(N, self.val_acc, label = "val_acc")
            plt.title("Training Loss and Accuracy [Epoch {}]".format(epoch))
            plt.xlabel("Epoch #")
            plt.ylabel("Loss/Accuracy")
            plt.legend()
            plt.show()
            plt.close()

TrainingPlot1 = TrainingPlot()

num_epochs = 50
history = model.fit(padded_train,
                    label_train,
                    epochs=num_epochs,
                    validation_data=(padded_test, label_test),
                    verbose=2,
                    callbacks = [TrainingPlot1, EarlyStopper1]
                    )

"""Model berhasil mendapatkan hasil accuracy dan validation accuracy sebesar lebih dari 90% untuk 3 kelas dari sebuah dataset"""